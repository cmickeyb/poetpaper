%% -*- fill-column: 120; -*-
\section{Circuit Breaker}
\label{sec_failure}

As you know, we cannot stop a validator with a compromised enclave from
winning a validation round. A compromised enclave can create a bogus
wait certificate with a very short duration that would ensure that it
wins a given round. Our goal with respect to compromised enclaves is to:

\begin{enumerate}
\item Make it as hard as possible to compromise the enclave
\item Ensure that a compromised enclave cannot compromise the
  correctness of the ledger
\item Ensure that the ledger can continue to progress (that is, correct
  valiators will continue to accept, validate and commit new
  transactions) in spite of a ``reasonable'' number of compromised enclaves
  (assume at most 1/3 compromised validators).
\end{enumerate}

Andrea's work addresses the first goal. The separation of
consensus and correctness testing addresses the second goal. That leaves
the third goal to be addressed.

We have proposed some form of limitation on the frequency with which a
given validator can win lotteries as a means to address the third
goal. Limiting the frequency of wins (with the assumption that a
reasonable number of validators are correct) ensures that correct
validators will be able to commit a significant proportion of the
blocks. That is, compromised nodes might be able to degrade performance
of the ledger by committing correct, but potentially "meaningless",
blocks, but compromised nodes cannot prevent progress by correct
validators.

Originally we described the method of limiting frequency as an "M of N"
policy. That is, a given validator could win at most M lotteries out of
N blocks. That policy would work for a more or less consistent pool of
validators. However, the population of validators constantly
changes. And the values of M and N depend heavily on the population
size.

As an alternative I'm proposing that we use a "1 sample z test" (see [1]
and [2] below for more information). A "1 sample z test" computes the
value of the standard deviation of a specific observation relative to
the expected value. So the z value computed for a set of observations is
the number of standard deviations the observations are from the
expectation. To be concrete, with a population of 1000 validators, the
probability of winning is 1/1000. That means that for a sample of 100000
blocks, a validator would be expected to claim 100 blocks. If we observe
that a validator has actually won 125 blocks, that would mean the z
value would be about 2.5. We can then use the z value to compute the
probability that a given validator is winning more often than it
should. For example, with 99.5\% confidence, we can say that The "1
sample z test" has many nice properties: It allows for variance in
winning. It is not constrained to a specific time period so it can apply
to both recent performance and historic performance. A correct validator
can apply the test before it claims a block to know if claiming the
block would put outside of acceptable behavior (which means we can
remove "false positives" where the algorithm would falsely determine
that a correct validator is behaving incorrectly).

I have run a number of simulations with 100000 blocks over a dynamic
population of validators with a mean of around 1000. When I run the "1
sample z test" with a correct validator and a 99.5\% confidence interval
(the validator should win about 1 block out of 1000), I get a false
positive in about 5\% of the runs. A validator winning at a rate of 2
blocks out of 1000 was caught in every run, generally within a few
hundred blocks. To be clear, the test means that we check the statistic
for every prefix of the chain (did the validator win too often in the
last 10 blocks? The last 11 blocks? The last N blocks?). That way we
can identify bad behavior at any scale.

There are several ways to configure the algorithm: 

\begin{itemize}
\item The number of observations required before any test is considered
  valid (a validator must win at least 3 blocks before the test is
  applied by default)

\item The amount of "history" we consider (the default is to examine the
  entire chain)

\item The confidence interval (the default is 95\% though I have found
  that for large validator pools, 99.5\% provides very good numbers)

\item The "initialization" period (how many blocks have to be committed
  before we start testing)
\end{itemize}

That last parameter makes testing a big pain. Since the initialization
of the ledger does not use the probabilistic sampling to get wait times,
it has completely bogus numbers for the population of validators. So
there need to be enough blocks in the ledger to make the sampling an
accurate representation of the number of validators. I'm still playing
with that number; trying to see if there is any other way handle start
up more rationally.

The implementation uses a cache of population estimates that is
maintained by the journal layer (PoetJournal). The cache is a dictionary
that maps a block id to the identity of the validator of the block, the
population estimate from the WaitCertificate and the previous block
identifier. When a validator claims a block, the test computes the z
value for that validator for every prefix of the chain (rooted at the
current block). I'm fairly certain there are more efficient ways to do
the computation (how far do we have to look back through history to
determine that this one last block pushed the z value over the
threshold). Though even for 100000 blocks the time to compute isn't too
bad (< 1 second).

I have two concerns with the algorithm:
\begin{itemize}
\item There is a potential for a feedback loop. Taken to the extreme,
  this algorithm would result in round robin behavior (which is not
  necessarily a bad thing). If a validator wins a lottery, it might take
  itself out of the population while it waits for its next turn. That
  has an effect on the population estimates. Lower population estimates
  allow for validators to win more often. So the validator re-joins when
  it would be allowed to win. I think the system would stabilize but I
  don't know what the characteristic would be (e.g. for 1000 actual
  validators that are all playing by the rules, the population estimates
  might actually be closer to 500).

\item Bad things could happen if the validator population changes very
  quickly. For example, say that we start with 1000 nodes and the
  network is partitioned with 500 in each half. Could we end up in a
  situation where *NO* node could win the next lottery? I've seen
  something like this in the initialization where the population
  estimate is much larger than the actual population. If enough blocks
  are committed then the population estimate adjusts enough to
  accommodate. However, blocks have to be committed before the
  adjustment can be made.
\end{itemize}

Reproducing both of these situations is nearly impossible with an actual
ledger. I need to build a more robust simulator to convince myself that
bad things won't happen.

%[1] http://www.cogsci.ucsd.edu/classes/SP07/COGS14/NOTES/binomial_ztest.pdf
%[2] http://www.statisticslectures.com/topics/onesamplezproportions/
